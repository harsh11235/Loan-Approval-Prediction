{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dbffc6",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction — Final (v2)\n",
    "\n",
    "Corrected end-to-end pipeline for local Jupyter/VS Code:\n",
    "\n",
    "- Smart Tkinter file picker with fallbacks\n",
    "- Raw-file cleaning (removes leading row numbers)\n",
    "- Robust target detection and mapping to 0/1\n",
    "- Preprocessing pipelines (compatible with scikit-learn >=1.4)\n",
    "- Train/test split with safeguards\n",
    "- Model training (Logistic Regression, Random Forest) and evaluation\n",
    "- Model saving (`best_loan_model.joblib`) and simple fairness check\n",
    "\n",
    "Run cells from top to bottom. If anything fails, rerun the smart loader cell first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe204de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306fe917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart dataset loader + cleaning (handles leading row numbers)\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "\n",
    "DEFAULT_PATH = \"/mnt/data/loan_approval_dataset.csv\"  # edit if needed\n",
    "PERSIST_PATH_FILE = \"dataset_path.txt\"\n",
    "\n",
    "# determine path\n",
    "if Path(PERSIST_PATH_FILE).exists():\n",
    "    p = Path(PERSIST_PATH_FILE).read_text().strip()\n",
    "    if p:\n",
    "        path = Path(p)\n",
    "    else:\n",
    "        path = Path(DEFAULT_PATH)\n",
    "else:\n",
    "    path = Path(DEFAULT_PATH)\n",
    "\n",
    "# Try Tkinter picker to allow re-selection when running interactively\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    chosen = filedialog.askopenfilename(title=\"Select your loan approval dataset (CSV)\", filetypes=[(\"CSV Files\",\"*.csv\"),(\"All Files\",\"*.*\")])\n",
    "    root.destroy()\n",
    "    if chosen:\n",
    "        path = Path(chosen)\n",
    "except Exception:\n",
    "    # ignore if running in environment without GUI\n",
    "    pass\n",
    "\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {path.resolve()}\")\n",
    "\n",
    "print('Reading raw file:', path.resolve())\n",
    "raw_text = path.read_text(encoding='utf-8', errors='replace')\n",
    "\n",
    "# Remove leading row numbers like '1 ' at start of each line\n",
    "cleaned_lines = [re.sub(r'^\\s*\\d+\\s+', '', line) for line in raw_text.splitlines()]\n",
    "cleaned_text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "# Read CSV\n",
    "from io import StringIO\n",
    "df_raw = pd.read_csv(StringIO(cleaned_text), skipinitialspace=True)\n",
    "# normalize column names\n",
    "df_raw.columns = [c.strip().lower() for c in df_raw.columns]\n",
    "\n",
    "print('Raw read shape:', df_raw.shape)\n",
    "display(df_raw.head())\n",
    "\n",
    "# Persist chosen path\n",
    "try:\n",
    "    with open(PERSIST_PATH_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(path.resolve()))\n",
    "    print('Saved dataset path to', PERSIST_PATH_FILE)\n",
    "except Exception as e:\n",
    "    print('Could not persist path:', e)\n",
    "\n",
    "# Expose df_raw for next cells\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target detection and robust mapping to 0/1\n",
    "candidate_targets = ['loan_status','loanstatus','status','approved','is_approved']\n",
    "target_col = None\n",
    "for t in candidate_targets:\n",
    "    if t in df.columns:\n",
    "        target_col = t\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    for c in df.columns:\n",
    "        if df[c].nunique() == 2:\n",
    "            target_col = c\n",
    "            break\n",
    "\n",
    "if target_col is None:\n",
    "    raise RuntimeError('Could not detect a target column. Columns: ' + ','.join(df.columns))\n",
    "\n",
    "print('Using target column:', target_col)\n",
    "print('Raw target value counts:')\n",
    "print(df[target_col].value_counts(dropna=False).head(20))\n",
    "\n",
    "def map_target(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    s = str(v).strip().lower()\n",
    "    if s in ('approved','approve','a','y','yes','1','true'): return 1\n",
    "    if s in ('rejected','reject','r','n','no','0','false'): return 0\n",
    "    try:\n",
    "        nv = float(s)\n",
    "        if nv==1.0: return 1\n",
    "        if nv==0.0: return 0\n",
    "    except: pass\n",
    "    return np.nan\n",
    "\n",
    "df['target'] = df[target_col].apply(map_target)\n",
    "print('\\nMapped target counts (including NaN):')\n",
    "print(df['target'].value_counts(dropna=False))\n",
    "\n",
    "before = len(df)\n",
    "df = df[df['target'].notna()].reset_index(drop=True)\n",
    "after = len(df)\n",
    "print(f'Dropped {before-after} rows with unmapped target. Remaining rows: {after}')\n",
    "\n",
    "y = df['target'].astype(int)\n",
    "X = df.drop(columns=[target_col, 'target'])\n",
    "print('Final X shape:', X.shape, 'y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names (strip) and basic type conversions\n",
    "X.columns = [c.strip().lower() for c in X.columns]\n",
    "print(X.dtypes)\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c909c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: create debt-to-income if applicable\n",
    "if {'income_annum','loan_amount'}.issubset(set(X.columns)):\n",
    "    X['dti_estimate'] = X['loan_amount'] / (X['income_annum'].replace(0, np.nan))\n",
    "    print('Created dti_estimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_samples = len(X)\n",
    "print('Total samples:', n_samples)\n",
    "if n_samples == 0:\n",
    "    raise ValueError('No samples available after processing target.')\n",
    "\n",
    "class_counts = y.value_counts()\n",
    "print('Class counts:', class_counts.to_dict())\n",
    "\n",
    "use_stratify = True\n",
    "if class_counts.min() < 2 or len(class_counts) != 2:\n",
    "    print('Disabling stratify due to small/odd class distribution.')\n",
    "    use_stratify = False\n",
    "\n",
    "test_size = 0.2\n",
    "if n_samples < 10:\n",
    "    test_size = max(1, int(np.floor(0.2 * n_samples)))\n",
    "    print('Small dataset — using test size (count):', test_size)\n",
    "\n",
    "if use_stratify:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=None)\n",
    "\n",
    "print('\\nTrain/Test shapes:')\n",
    "print('X_train:', X_train.shape, '| X_test:', X_test.shape)\n",
    "print('y_train:', y_train.shape, '| y_test:', y_test.shape)\n",
    "\n",
    "print('\\nTrain class distribution:')\n",
    "print(pd.Series(y_train).value_counts().to_dict())\n",
    "print('Test class distribution:')\n",
    "print(pd.Series(y_test).value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines (numeric + categorical)\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "print('Numeric cols:', num_cols)\n",
    "print('Categorical cols:', cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4872de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models: Logistic Regression and Random Forest\n",
    "models = {}\n",
    "models['logreg'] = Pipeline([('pre', preprocessor),('clf', LogisticRegression(max_iter=1000, random_state=42))])\n",
    "models['rf'] = Pipeline([('pre', preprocessor),('clf', RandomForestClassifier(n_estimators=200, random_state=42))])\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    print('Training', name)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5428908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results = {}\n",
    "for name, pipe in models.items():\n",
    "    preds = pipe.predict(X_test)\n",
    "    probs = pipe.predict_proba(X_test)[:,1] if hasattr(pipe, 'predict_proba') else None\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs) if probs is not None else None\n",
    "    print(f\"\\n{name} - acc: {acc:.4f}, f1: {f1:.4f}, auc: {auc}\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.show()\n",
    "    results[name] = {'acc':acc,'f1':f1,'auc':auc,'model':pipe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model by F1 then AUC and save\n",
    "best_name = max(results.keys(), key=lambda k: (results[k]['f1'], results[k]['auc'] if results[k]['auc'] is not None else 0))\n",
    "best_model = results[best_name]['model']\n",
    "print('Best model:', best_name, results[best_name])\n",
    "joblib.dump({'model': best_model, 'features': X.columns.tolist(), 'target_col': target_col}, 'best_loan_model.joblib')\n",
    "print('Saved best model to best_loan_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fairness check\n",
    "protected = None\n",
    "for p in ['gender','sex','age','married']:\n",
    "    if p in X_test.columns:\n",
    "        protected = p\n",
    "        break\n",
    "\n",
    "if protected:\n",
    "    preds = best_model.predict(X_test)\n",
    "    df_fair = X_test.reset_index(drop=True).copy()\n",
    "    df_fair['pred'] = preds\n",
    "    rates = df_fair.groupby(protected)['pred'].mean()\n",
    "    print('Approval rates by', protected)\n",
    "    print(rates)\n",
    "    di = rates.min()/(rates.max()+1e-9)\n",
    "    print('Disparate impact:', di)\n",
    "else:\n",
    "    print('No common protected attribute present (gender/age/married)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo prediction\n",
    "sample = X_test.sample(1, random_state=1)\n",
    "print('Sample:')\n",
    "display(sample.head())\n",
    "print('Predicted approval:', best_model.predict(sample)[0])\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    print('Approval probability:', best_model.predict_proba(sample)[:,1][0])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
